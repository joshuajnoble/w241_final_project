---
title: "R Notebook"
output: html_notebook
---

```{r}

#install.packages ("purrr")
```


```{r}
library(data.table)

library(sandwich)
library(lmtest)

library(AER) 

library (purrr)

library(ggplot2) 
library(patchwork)
library(stargazer)
library(dplyr)
library(janitor)
```

```{r}
d <- fread("dataset_tmp.csv")
```


```{r}

d[,q1_right := ifelse(Chart_Control1 == 'Right' | Chart_Control2 == 'Right' | Chart_Treat1 == 'Right' | Chart_Treat2 == 'Right', 1, 0) ]

d[,q1_treat := ifelse( Chart_Control1 == "" & Chart_Control1 == "", 1, 0) ]

d[,q2_right := ifelse(Array_Control1 == 'Right' | Array_Control2 == 'Right' | Array_Control3 == 'Right' | Array_Treat1 == 'Right' | Array_Treat2 == 'Right' | Array_Treat3 == 'Right', 1, 0) ]

d[,q2_treat := ifelse( Array_Treat2  != "" | Array_Treat1  != "" | Array_Treat3 != "", 1, 0) ]

d[,all_q_right := q1_right + q2_right ]

d[,esl_treat := ifelse( ESL == "Yes"  & (q2_treat == 1 | q1_treat == 1), 1, 0) ]

tabyl(d, q1_right, q2_right)
```

Got the first chart wrong:

```{r}
chart1_wrong <- count(d[q1_right == 0 & q1_treat == 1,]) 
chart1_wrong / count(d[q1_treat == 1,])
```

```{r}
chart2_wrong <- count(d[q2_right == 0 & q2_treat == 1,])
chart2_wrong / count(d[q2_treat == 1])
```

So chart1 is almost 5x harder than chart2. This makes sense: pie charts suck. Let's look at simple stuff:


```{r}
simple_model <- lm(all_q_right ~ q1_treat + q2_treat, data = d)
stargazer(simple_model,
          se = list(sqrt(diag(vcovHC(simple_model)))),
          type = 'text',
          header = F)
```

q1 is definitely harder as a chart. q2 is definitely easier as a chart.

```{r}
esl_model <- lm(all_q_right ~ q1_treat + q2_treat + esl_treat + as.factor(ESL == "Yes"), data = d)

stargazer(esl_model,
          se = list(sqrt(diag(vcovHC(esl_model)))),
          type = 'text',
          header = F)
```

Q1 is harder for treatment, Q2 is not. That makes sense because Pie Charts without labels are awful. Adding in ESL shows that ESL doesn't make the treatment any better, or maybe it's treatment doesn't make ESL any better. Being ESL does actually improve the overall results though interestingly.

How does ESL affect the two questions separately? We'll look at interaction effects:

```{r}
q1_esl_model <- lm(q1_right ~ q1_treat * as.factor(ESL == 'Yes'), data = d)

q2_esl_model <- lm(q2_right ~ q2_treat * as.factor(ESL == 'Yes'), data = d)

stargazer(q1_esl_model, q2_esl_model,
          #se = list(sqrt(diag(vcovHC(q1_esl_model)))),
          type = 'text',
          header = F)
```

 Looks like Q1 is just as confusing for ESL as non-ESL, but Q2 looks much worse.


```{r}
esl_model <- ivreg(all_q_right ~  as.factor(ESL == "Yes"), data = d)
```

 
```{r}
proportions_table = table(d[ESL=="No",], d[ESL=="Yes",])
chisq.test(proportions_table)
```
 
Looks like our ESL folks are statistically the same as our non. How about our chart strugglers?

```{r}
proportions_table_2 = table(d[q1_treat == 1 & q1_right == 0,], d[q2_treat == 1 & q2_right == 0,])
chisq.test(proportions_table_2)
```

Also, different. We only had one person attrit so there's no need to do a CACE or attrition calculation.