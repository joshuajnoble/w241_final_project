---
title: "w214 Final Project"
author: "Joshua Noble, Ryan T Orton, Sandip Panesar"
date: "4/8/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Icons, Charts, Denominators
### Joshua Noble, Ryan T. Orton, Sandip Panesar
### W241 Spring 2021


## Introduction

Reading and understanding ratios, particularly when presented numerically, can be a taxing task even for people who claim to be at ease with mathematics **Reference 1**. The entire field of data visualization has been built around the premise that humans count objects and see ratios better when presented with graphical representations of those quantities rather than numerical ones. We wanted to test whether this was true in a very specific context reading ratios. Many ratios seem quite easy to read and understand e.g. 1/2, 25%, ‘2 out of 3’, but many others are more difficult to interpret. Consequently, it is more difficult to process their context, particularly when related to probability or efficacy. There are numerous examples of this effect in everyday life: Millions of people buy lottery tickets despite the odds; random sampling is foreign enough that companies still use it as a brain-teaser question in job interviews; and as we’ve seen in the past six months, vaccine efficacy is rather difficult to understand.

## Study Rationale and Hypothesis

Vaccine efficacy is measured by calculating the risk of disease among vaccinated and unvaccinated persons and determining the percentage reduction in risk of disease among vaccinated persons relative to unvaccinated persons. The greater the percentage reduction of illness in the vaccinated group, the greater the vaccine efficacy. The basic formula is written as:

$$
\mathrm{Vaccine\ Efficacy\ =\ \frac{Risk\ among\ unvaccinated\ group\ -\ Risk\ among\ vaccinated\ group}{Risk\ among\ unvaccinated\ group}}
$$

It is unclear exactly how well this is understood, however. Certainly the COVID-19 pandemic has provided us ample opportunity to witness how these efficacy results can be confusing.

According to Garcia-Retamero et al **Reference 2**, denominator neglect is the focus on the number of times a target event has happened (e.g. the number of treated and nontreated patients who die) without considering the overall number of opportunities for it to happen (e.g. the overall number of treated and nontreated patients). In their 2 studies, the authors' addressed the effect of denominator neglect in problems involving treatment risk reduction where samples of treated and non-treated patients and the relative risk reduction were of different sizes. They also tested whether using icon arrays helped people take these different sample sizes into account. 

Based upon the above, our causal research question is:

**Are icon arrays (or other visual aids) more effective than text-based numerical ratios for interpretation of numerical ratios?**

We hypothesize, based upon the aforementioned evidence that they are. More specifically, that a group presented with icon arrays correctly interprets the numerical ratios presented in them at a greater rate, compared to a group presented with only text or numbers.

## Methods

The original paper on which our study was based examined numerical ratios and icon arrays. In order to expand the test we wanted to test both a chart that required interpretation, an icon array which might require counting, and a numerical ratio which showed a vaccine efficacy. Our experiment was conducted using the Berkeley XLab to recruit survey takers.

## Research Design

We adopted an alternative approach to the classical pre-test/post-test ROXO and instead used a RXRXO post-test control group design. Our survey did not involve any observation prior to treatment or control randomization. The subjects were tested via a multi-factorial approach: All users were first presented with several "stem" questions about the COVID-19 pandemic and children returning to classrooms. These questions were the same for all participants and were intended to obscure the actual purpose of our survey so that participants wouldn't immediately think of it as testing ratios or mathematical concepts. Participants were first presented with a question from the chart treatment stage: Either a numerical description or a chart. Then the same subject was presented with a question from the icon array stage: Either a text-based description or an icon array. Within each stage, subjects would then be assigned randomly (using Qualtrics' survey flow randomization algorithm) into either control or treatment groups (**Figure 1.**):

![Figure 1.](images/roxo.png)
**Figure 1** A diagram demonstrating the overall RXRXO study design.

### Pie Chart Questions

Pie charts are a particularly ineffective form of visualization **Reference 3**. Our method involved participants differentiating between pie charts. In information visualization, when presented with several pie charts, subjects often have difficult interpreting or differentiating between them **Reference 4**. The pie charts we utilized lacked numbers and used ratios that were deliberately intended to be difficult to visually decode: 80%, 90%, 95%, and 97%. In this arm, the pie chart was either right or wrong, depending upon the ratio presented. If a pie chart without labels is more effective than a numerical ratio at expressing efficacy then this provides us with alternative evidence to test the hypothesis that visual aids were still more effective than either text or numerical ratios **Figure 2**

![Figure 2a.](images/3_97.png){width=50%}
![Figure 2b.](images/5_95.png){width=50%}

**Figure 2** a. A pie chart representing 97% efficacy, b. A pie chart representing 85% efficacy.

### Icon Array Questions

For the icon array questions, icon arrays were employed to test numerical ratios of efficacy against icon arrays that represented vaccine efficacy. These icon arrays differ from the pie chart pairings in that they contain countable quantities of different colored icons and thus can be more precise than the charts **Figure 3**. 

![Figure 3a.](images/array_1.png){width=50%}

![Figure 3b.](images/array_2.png){width=50%}

**Figure 3** a. Icon array representing the treatment group of a hypothetical vaccine trial, whereby 4 out of 200 (represented by red and black figures, respectively) contracted COVID-19 following vaccination. b. Icon array representing the control group of a hypothetical vaccine trial, whereby 10 out of 100 (represented by red and black figures, respectively) contracted COVID-19 following vaccination. Altogether, vaccine efficacy is 80%.

The 'correct' answer to the question presented to the icon array subjects is also significantly easier to interpret compared to those in the chart arm. The correct answer was always 'yes', i.e. the vaccine was efficacious at the 80% level.

## Confounding Variables

In addition to the study-specific data, we also acquired a range of demographic data of participants. Numerous factors may affect the responses to our questions, namely how well subjects were able to understand them. These include age, level of educational attainment and English as a second language (ESL) status. We therefore kept data from the Berkeley Xlab experimental survey pertaining to these factors for use in analysis. 

## Analytic Plan

> 1. Descriptive statistics.
> 2. Group-based Statistical Tests (Chi-Squared).
> 3. Regression Analysis with and without potential control features. 

## Pilot Data

We ran a pilot study using Qualitrics which received 19 valid responses. This survey was open for approximately 10 days. The structure of the Pilot survey contained two arms which tested a chart against a numerator/denominator numerical representation and an icon array against a numerator/denominator numerical representation. We didn't collect demographic data from respondents. The accuracy of responses to the two questions is shown below: 

```{r}
pilot <- fread("pilot_data.csv")

pilot[,q1_right := ifelse((Chart_Control1 == 'Right' | Chart_Control2 == 'Right' | Chart_Treat1 == 'Right' | Chart_Treat2 == 'Right'), 1, 0) ]
pilot[,q1_treat := ifelse( (Chart_Treat1 != "" | Chart_Treat2 != ""), 1, 0) ]
pilot[,q2_right := ifelse(Array_Control1 == 'Right' | Array_Control2 == 'Right' | Array_Control3 == 'Right' | Array_Treat1 == 'Right' | Array_Treat2 == 'Right' | Array_Treat3 == 'Right', 1, 0) ]
pilot[,q2_treat := ifelse( Array_Treat2  != "" | Array_Treat1  != "" | Array_Treat3 != "", 1, 0) ]
pilot[,all_q_right := q1_right + q2_right ]

tabyl(pilot, q1_right, q2_right)
```

We can assess the users accuracy with each treatment by evaluating how often they correctly answered  the questions in the pilot:

```{r}
p_chart_treat_right <- nrow(pilot[q1_right == 1 & q1_treat == 1,]) / nrow(pilot[q1_treat == 1,]) 
p_chart_control_right <- nrow(pilot[q1_right == 1 & q1_treat == 0,]) / nrow(pilot[q1_treat == 0,]) 
```

In the chart treatment group, users answered correctly `r p_chart_treat_right`% of the time, while in the control they answered `r p_chart_control_right`.

```{r}
p_array_treat_right <- nrow(pilot[q2_right == 1 & q2_treat == 1,]) / nrow(pilot[q2_treat == 1,])
p_array_control_right <- nrow(pilot[q2_right == 1 & q2_treat == 0,]) / nrow(pilot[q2_treat == 0,])

p_array_treat_right
p_array_control_right
```

In the chart treatment group, users answered correctly `r p_array_treat_right`% of the time, while in the control they answered `r p_array_control_right` **Table 1.**.

![Table 1.](images/pilot_table.png)


The pilot study was not large enough to provide reliable sample sizes but it did provide confirmation that our survey itself was working as intended.

## Results 

The overall breakdown of our study is provided in **Figure 4.**. In all there were 309 respondents. 45 subjects were lost to attrition (test-stage surveys, failure to complete). For the first question, 131 subjects were presented with the pie-chart (treatment group) and 133 subjects were presented with the text control question. For the second question, 128 subjects were presented with the icon array (treatment group), 128 subjects were presented with the text control question. 

![Figure 4.](images/StudyFlow.png)

**Figure 4.** A diagram demonstrating the study flow and numbers of respondents for each respective control and treatment stage. 

### Stem Questions

These were asked mainly as a distractor for subjects, prior to being presented with the questions of interest. The data is presented in **Table 2.**

![Table 2.](images/stems.png) 

### Demographics

Regarding relevant demographics, participant age ranged from 18 to 54, with a mean age of 23.5 (standard deviation 5.5). Relevant demographics of the test subjects are presented, grouped by educational level in **Table 3.**.

![Table 3.](images/demographics.png) 

Approximately half of the respondents had completed some college (likely undergraduate students), a quarter had completed their undergraduate studies, while only 8% progressed onto further education. 16% of respondents had no college. Interestingly, the majority of respondents in all educational groups were ESL students, with over 4/5 of postgraduate respondents reporting ESL status. 

### Between-Group Comparison

**Table 4.** summarizes the responses received for the first set of questions, where the subjects were exposed to either the text or the pie chart. 83.5% of those in the control answered the question correctly, while only 68.9% in the treatment did. As these data were categorical, consisted of counts between groups, and were generated through random sampling we utilized a $\chi^2$ test to determine if the proportion of right and wrong answers was significantly different between groups. The $\chi^2$ value was 7.42 and had a p-value of 0.006, indicating a highly significant difference. 

![Table 4.](images/chart_matrix.png) 

**Table 5.** summarizes the responses received for the second set of questions, where the subjects were exposed to either the text or the icon array. 84.4% of those in the control answered the question correctly, while 93.8% in the treatment did. As these data were categorical, consisted of counts between groups, and were generated through random sampling we utilized a $\chi^2$ test to determine if the proportion of right and wrong answers was significantly different between groups. The $\chi^2$ value was 5.80 and had a p-value of 0.0160, indicating a significant difference at the 5% level. 

![Table 5.](images/array_matrix.png)

To summarize, the treatment arm of the chart control group had a higher proportion of incorrect answers compared to the control arm, while the treatment arm of the icon array group had a higher proportion of correct answers compared to the controls. These differences were significant at the 5% level at least, confirming our original hypothesis. Nevertheless, the $\chi^2$ approach cannot control for the additional demographic factors that we identified as potentially being relevant, so regression analysis may offer further insight by allowing us to control for these factors.  

### Regression Analysis

We created a simple model that regresses the number of correct answers against the participants being in two treatment groups:

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & all\_q\_right \\ 
\hline \\[-1.8ex] 
 q1\_treat & $-$0.014 \\ 
  & (0.070) \\ 
  & \\ 
 q2\_treat & 0.150$^{**}$ \\ 
  & (0.069) \\ 
  & \\ 
 Constant & 1.551$^{***}$ \\ 
  & (0.064) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & 264 \\ 
R$^{2}$ & 0.018 \\ 
Adjusted R$^{2}$ & 0.011 \\ 
Residual Std. Error & 0.557 (df = 261) \\ 
F Statistic & 2.454$^{*}$ (df = 2; 261) \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} \


Our second model regresses the response for each arm of the study against being in the respective treatment group and the education level of the participant.

\begin{table}[!htbp] \centering 
  \caption{} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lcc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & q1\_right & q2\_right \\ 
\\[-1.8ex] & (1) & (2)\\ 
\hline \\[-1.8ex] 
 q1\_treat & $-$0.087 &  \\ 
  & (0.055) &  \\ 
  & & \\ 
 q2\_treat &  & 0.110$^{***}$ \\ 
  &  & (0.039) \\ 
  & & \\ 
 as.factor(Edu\_Level)Bachelor's degree & 0.035 & $-$0.072 \\ 
  & (0.110) & (0.078) \\ 
  & & \\ 
 as.factor(Edu\_Level)No college & 0.093 & $-$0.204$^{**}$ \\ 
  & (0.118) & (0.083) \\ 
  & & \\ 
 as.factor(Edu\_Level)Some college & 0.178$^{*}$ & $-$0.136$^{*}$ \\ 
  & (0.103) & (0.073) \\ 
  & & \\ 
 Constant & 0.656$^{***}$ & 0.953$^{***}$ \\ 
  & (0.099) & (0.070) \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & 264 & 264 \\ 
R$^{2}$ & 0.031 & 0.054 \\ 
Adjusted R$^{2}$ & 0.016 & 0.039 \\ 
Residual Std. Error (df = 259) & 0.441 & 0.312 \\ 
F Statistic (df = 4; 259) & 2.087$^{*}$ & 3.678$^{***}$ \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

## References

1. **Insert reference from intro paragraph here**
2. [Do Icon Arrays Help Reduce Denominator Neglect?]
3. [(William S. Cleveland, The Elements of Graphing Data, Hobart Press, 1994]
4. (Edward Tufte, The Visual Display of Quantitative Information, Graphics Press, 1983, p. 178.)

